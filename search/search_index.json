{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyLLaMACpp API Reference","text":""},{"location":"#pyllamacpp.model","title":"pyllamacpp.model","text":"<p>This module contains a simple Python API around llama.cpp</p>"},{"location":"#pyllamacpp.model.Model","title":"Model","text":"<pre><code>Model(\n    model_path,\n    prompt_context=\"\",\n    prompt_prefix=\"\",\n    prompt_suffix=\"\",\n    log_level=logging.ERROR,\n    n_ctx=512,\n    seed=0,\n    n_gpu_layers=0,\n    f16_kv=False,\n    logits_all=False,\n    vocab_only=False,\n    use_mlock=False,\n    embedding=False,\n)\n</code></pre> <p>A simple Python class on top of llama.cpp</p> <p>Example usage <pre><code>from pyllamacpp.model import Model\n\nmodel = Model(ggml_model='path/to/ggml/model')\nfor token in model.generate(\"Tell me a joke ?\"):\n    print(token, end='', flush=True)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>the path to the ggml model</p> required <code>prompt_context</code> <code>str</code> <p>the global context of the interaction</p> <code>''</code> <code>prompt_prefix</code> <code>str</code> <p>the prompt prefix</p> <code>''</code> <code>prompt_suffix</code> <code>str</code> <p>the prompt suffix</p> <code>''</code> <code>log_level</code> <code>int</code> <p>logging level, set to INFO by default</p> <code>logging.ERROR</code> <code>n_ctx</code> <code>int</code> <p>LLaMA context</p> <code>512</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>0</code> <code>n_gpu_layers</code> <code>int</code> <p>number of layers to store in VRAM</p> <code>0</code> <code>f16_kv</code> <code>bool</code> <p>use fp16 for KV cache</p> <code>False</code> <code>logits_all</code> <code>bool</code> <p>the llama_eval() call computes all logits, not just the last one</p> <code>False</code> <code>vocab_only</code> <code>bool</code> <p>only load the vocabulary, no weights</p> <code>False</code> <code>use_mlock</code> <code>bool</code> <p>force system to keep model in RAM</p> <code>False</code> <code>embedding</code> <code>bool</code> <p>embedding mode only</p> <code>False</code> Source code in <code>pyllamacpp/model.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             prompt_context: str = '',\n             prompt_prefix: str = '',\n             prompt_suffix: str = '',\n             log_level: int = logging.ERROR,\n             n_ctx: int = 512,\n             seed: int = 0,\n             n_gpu_layers: int = 0,\n             f16_kv: bool = False,\n             logits_all: bool = False,\n             vocab_only: bool = False,\n             use_mlock: bool = False,\n             embedding: bool = False):\n\"\"\"\n    :param model_path: the path to the ggml model\n    :param prompt_context: the global context of the interaction\n    :param prompt_prefix: the prompt prefix\n    :param prompt_suffix: the prompt suffix\n    :param log_level: logging level, set to INFO by default\n    :param n_ctx: LLaMA context\n    :param seed: random seed\n    :param n_gpu_layers: number of layers to store in VRAM\n    :param f16_kv: use fp16 for KV cache\n    :param logits_all: the llama_eval() call computes all logits, not just the last one\n    :param vocab_only: only load the vocabulary, no weights\n    :param use_mlock: force system to keep model in RAM\n    :param embedding: embedding mode only\n    \"\"\"\n\n    # set logging level\n    set_log_level(log_level)\n    self._ctx = None\n\n    if not Path(model_path).is_file():\n        raise Exception(f\"File {model_path} not found!\")\n\n    self.llama_params = pp.llama_context_default_params()\n    # update llama_params\n    self.llama_params.n_ctx = n_ctx\n    self.llama_params.seed = seed\n    self.llama_params.n_gpu_layers = n_gpu_layers\n    self.llama_params.f16_kv = f16_kv\n    self.llama_params.logits_all = logits_all\n    self.llama_params.vocab_only = vocab_only\n    self.llama_params.use_mlock = use_mlock\n    self.llama_params.embedding = embedding\n\n    self._ctx = pp.llama_init_from_file(model_path, self.llama_params)\n\n    # gpt params\n    self.gpt_params = pp.gpt_params()\n\n    self.res = \"\"\n\n    self._n_ctx = pp.llama_n_ctx(self._ctx)\n    self._last_n_tokens = [0] * self._n_ctx  # n_ctx elements\n    self._n_past = 0\n    self.prompt_cntext = prompt_context\n    self.prompt_prefix = prompt_prefix\n    self.prompt_suffix = prompt_suffix\n\n    self._prompt_context_tokens = []\n    self._prompt_prefix_tokens = []\n    self._prompt_suffix_tokens = []\n\n    self.reset()\n</code></pre>"},{"location":"#pyllamacpp.model.Model.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets the context</p> Source code in <code>pyllamacpp/model.py</code> <pre><code>def reset(self) -&gt; None:\n\"\"\"Resets the context\"\"\"\n    self._prompt_context_tokens = pp.llama_tokenize(self._ctx, self.prompt_cntext, True)\n    self._prompt_prefix_tokens = pp.llama_tokenize(self._ctx, self.prompt_prefix, True)\n    self._prompt_suffix_tokens = pp.llama_tokenize(self._ctx, self.prompt_suffix, True)\n    self._last_n_tokens = [0] * self._n_ctx  # n_ctx elements\n    self._n_past = 0\n</code></pre>"},{"location":"#pyllamacpp.model.Model.tokenize","title":"tokenize","text":"<pre><code>tokenize(text)\n</code></pre> <p>Returns a list of tokens for the text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text to be tokenized</p> required <p>Returns:</p> Type Description <p>List of tokens</p> Source code in <code>pyllamacpp/model.py</code> <pre><code>def tokenize(self, text: str):\n\"\"\"\n    Returns a list of tokens for the text\n    :param text: text to be tokenized\n    :return: List of tokens\n    \"\"\"\n    return pp.llama_tokenize(self._ctx, text, True)\n</code></pre>"},{"location":"#pyllamacpp.model.Model.detokenize","title":"detokenize","text":"<pre><code>detokenize(tokens)\n</code></pre> <p>Returns a list of tokens for the text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text to be tokenized</p> required <p>Returns:</p> Type Description <p>A string representing the text extracted from the tokens</p> Source code in <code>pyllamacpp/model.py</code> <pre><code>def detokenize(self, tokens: list):\n\"\"\"\n    Returns a list of tokens for the text\n    :param text: text to be tokenized\n    :return: A string representing the text extracted from the tokens\n    \"\"\"\n    return pp.llama_tokens_to_str(self._ctx, tokens)\n</code></pre>"},{"location":"#pyllamacpp.model.Model.generate","title":"generate","text":"<pre><code>generate(\n    prompt,\n    n_predict=None,\n    n_threads=4,\n    seed=None,\n    antiprompt=None,\n    n_batch=512,\n    n_keep=0,\n    top_k=40,\n    top_p=0.95,\n    tfs_z=1.0,\n    typical_p=1.0,\n    temp=0.8,\n    repeat_penalty=1.1,\n    repeat_last_n=64,\n    frequency_penalty=0.0,\n    presence_penalty=0.0,\n    mirostat=0,\n    mirostat_tau=5.0,\n    mirostat_eta=0.1,\n    infinite_generation=False,\n)\n</code></pre> <p>Runs llama.cpp inference and yields new predicted tokens from the prompt provided as input</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt :)</p> required <code>n_predict</code> <code>Union[None, int]</code> <p>if n_predict is not None, the inference will stop if it reaches <code>n_predict</code> tokens, otherwise it will continue until <code>EOS</code></p> <code>None</code> <code>n_threads</code> <code>int</code> <p>The number of CPU threads</p> <code>4</code> <code>seed</code> <code>Union[None, int]</code> <p>Set rng seed, leave it None for random</p> <code>None</code> <code>antiprompt</code> <code>str</code> <p>aka the stop word, the generation will stop if this word is predicted, keep it None to handle it in your own way</p> <code>None</code> <code>n_batch</code> <code>int</code> <p>batch size for prompt processing (must be &gt;=32 to use BLAS)</p> <code>512</code> <code>n_keep</code> <code>int</code> <p>number of tokens to keep from initial prompt</p> <code>0</code> <code>top_k</code> <code>int</code> <p>top K sampling parameter, &lt;= 0 to use vocab size</p> <code>40</code> <code>top_p</code> <code>float</code> <p>top P sampling parameter, 1.0 = disabled</p> <code>0.95</code> <code>tfs_z</code> <code>float</code> <p>tfs_z sampling parameter, 1.0 = disabled</p> <code>1.0</code> <code>typical_p</code> <code>float</code> <p>typical_p sampling parameter, 1.0 = disabled</p> <code>1.0</code> <code>temp</code> <code>float</code> <p>Temperature, 1.0 = disabled</p> <code>0.8</code> <code>repeat_penalty</code> <code>float</code> <p>repeat penalty sampling parameter, 1.0 = disabled</p> <code>1.1</code> <code>repeat_last_n</code> <code>int</code> <p>last n tokens to penalize (0 = disable penalty, -1 = context size)</p> <code>64</code> <code>frequency_penalty</code> <code>float</code> <p>0.0 = disabled</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>0.0 = disabled</p> <code>0.0</code> <code>mirostat</code> <code>int</code> <p>0 = disabled, 1 = mirostat, 2 = mirostat 2.0</p> <code>0</code> <code>mirostat_tau</code> <code>int</code> <p>target entropy</p> <code>5.0</code> <code>mirostat_eta</code> <code>int</code> <p>learning rate</p> <code>0.1</code> <code>infinite_generation</code> <code>bool</code> <p>set it to <code>True</code> to make the generation go infinitely</p> <code>False</code> <p>Returns:</p> Type Description <code>Generator</code> <p>Tokens generator</p> Source code in <code>pyllamacpp/model.py</code> <pre><code>def generate(self,\n             prompt: str,\n             n_predict: Union[None, int] = None,\n             n_threads: int = 4,\n             seed: Union[None, int] = None,\n             antiprompt: str = None,\n             n_batch: int = 512,\n             n_keep: int = 0,\n             top_k: int = 40,\n             top_p: float = 0.95,\n             tfs_z: float = 1.00,\n             typical_p: float = 1.00,\n             temp: float = 0.8,\n             repeat_penalty: float = 1.10,\n             repeat_last_n: int = 64,\n             frequency_penalty: float = 0.00,\n             presence_penalty: float = 0.00,\n             mirostat: int = 0,\n             mirostat_tau: int = 5.00,\n             mirostat_eta: int = 0.1,\n             infinite_generation: bool = False) -&gt; Generator:\n\"\"\"\n    Runs llama.cpp inference and yields new predicted tokens from the prompt provided as input\n\n    :param prompt: The prompt :)\n    :param n_predict: if n_predict is not None, the inference will stop if it reaches `n_predict` tokens, otherwise\n                      it will continue until `EOS`\n    :param n_threads: The number of CPU threads\n    :param seed: Set rng seed, leave it None for random\n    :param antiprompt: aka the stop word, the generation will stop if this word is predicted,\n                       keep it None to handle it in your own way\n    :param n_batch: batch size for prompt processing (must be &gt;=32 to use BLAS)\n    :param n_keep: number of tokens to keep from initial prompt\n    :param top_k: top K sampling parameter, &lt;= 0 to use vocab size\n    :param top_p: top P sampling parameter, 1.0 = disabled\n    :param tfs_z: tfs_z sampling parameter, 1.0 = disabled\n    :param typical_p: typical_p sampling parameter, 1.0 = disabled\n    :param temp: Temperature, 1.0 = disabled\n    :param repeat_penalty: repeat penalty sampling parameter, 1.0 = disabled\n    :param repeat_last_n: last n tokens to penalize (0 = disable penalty, -1 = context size)\n    :param frequency_penalty: 0.0 = disabled\n    :param presence_penalty: 0.0 = disabled\n    :param mirostat: 0 = disabled, 1 = mirostat, 2 = mirostat 2.0\n    :param mirostat_tau: target entropy\n    :param mirostat_eta: learning rate\n    :param infinite_generation: set it to `True` to make the generation go infinitely\n\n    :return: Tokens generator\n    \"\"\"\n    # update params\n    self.gpt_params.n_batch = n_batch\n    self.gpt_params.n_keep = n_keep\n    self.gpt_params.top_k = top_k\n    self.gpt_params.top_p = top_p\n    self.gpt_params.tfs_z = tfs_z\n    self.gpt_params.typical_p = typical_p\n    self.gpt_params.temp = temp\n    self.gpt_params.repeat_penalty = repeat_penalty\n    self.gpt_params.repeat_last_n = repeat_last_n\n    self.gpt_params.frequency_penalty = frequency_penalty\n    self.gpt_params.presence_penalty = presence_penalty\n    self.gpt_params.mirostat = mirostat\n    self.gpt_params.mirostat_tau = mirostat_tau\n    self.gpt_params.mirostat_eta = mirostat_eta\n\n    if seed is not None:\n        pp.llama_set_rng_seed(self._ctx, seed)\n    else:\n        seed = int(time.time())\n        pp.llama_set_rng_seed(self._ctx, seed)\n\n    input_tokens = self._prompt_prefix_tokens + pp.llama_tokenize(self._ctx, prompt,\n                                                                  True) + self._prompt_suffix_tokens\n    if len(input_tokens) &gt; self._n_ctx - 4:\n        raise Exception('Prompt too long!')\n    predicted_tokens = []\n    predicted_token = 0\n\n    # add global context for the first time\n    # if self._n_past == 0:\n    for tok in self._prompt_context_tokens:\n        predicted_tokens.append(tok)\n        self._last_n_tokens.pop(0)\n        self._last_n_tokens.append(tok)\n\n    # consume input tokens\n    for tok in input_tokens:\n        predicted_tokens.append(tok)\n        self._last_n_tokens.pop(0)\n        self._last_n_tokens.append(tok)\n\n    n_remain = 0\n    if antiprompt is not None:\n        sequence_queue = []\n        stop_word = antiprompt.strip()\n\n    n_ctx = pp.llama_n_ctx(self._ctx)\n\n    while infinite_generation or predicted_token != pp.llama_token_eos():\n        if len(predicted_tokens) &gt; 0:\n            # infinite text generation via context swapping\n            if (self._n_past + len(predicted_tokens)) &gt; n_ctx:\n                n_left = self._n_past - self.gpt_params.n_keep\n                self._n_past = max(1, self.gpt_params.n_keep)\n                predicted_tokens[:0] = self._last_n_tokens[n_ctx - n_left // 2 - len(predicted_tokens):len(self._last_n_tokens) - len(predicted_tokens)]\n\n            for i in range(0, len(predicted_tokens), self.gpt_params.n_batch):\n                n_eval = len(predicted_tokens) - i\n                if n_eval &gt; self.gpt_params.n_batch:\n                    n_eval = self.gpt_params.n_batch\n\n                if (pp.llama_eval(self._ctx,\n                                  predicted_tokens[i:],\n                                  n_eval,\n                                  self._n_past,\n                                  n_threads)):\n                    raise Exception(\"Model eval failed!\")\n                self._n_past += n_eval\n\n        predicted_tokens.clear()\n\n        # sampling\n        predicted_token = pp.sample_next_token(self._ctx, self.gpt_params, self._last_n_tokens)\n\n        predicted_tokens.append(predicted_token)\n        # tokens come as raw undecoded bytes,\n        # and we decode them, replacing those that can't be decoded.\n        # I decoded here for fear of breaking the stopword logic,\n        token_str = pp.llama_token_to_str(self._ctx, predicted_token).decode('utf-8', \"replace\")\n        if antiprompt is not None:\n            if token_str == '\\n':\n                sequence_queue.append(token_str)\n                continue\n            if len(sequence_queue) != 0:\n                if stop_word.startswith(''.join(sequence_queue).strip()):\n                    sequence_queue.append(token_str)\n                    if ''.join(sequence_queue).strip() == stop_word:\n                        break\n                    else:\n                        continue\n                else:\n                    # consume sequence queue tokens\n                    while len(sequence_queue) != 0:\n                        yield sequence_queue.pop(0)\n                    sequence_queue = []\n        self._last_n_tokens.pop(0)\n        self._last_n_tokens.append(predicted_token)\n        if n_predict is not None:\n            if n_remain == n_predict:\n                break\n            else:\n                n_remain += 1\n        yield token_str\n</code></pre>"},{"location":"#pyllamacpp.model.Model.cpp_generate","title":"cpp_generate","text":"<pre><code>cpp_generate(\n    prompt,\n    n_predict=128,\n    new_text_callback=None,\n    n_threads=4,\n    top_k=40,\n    top_p=0.95,\n    tfs_z=1.0,\n    typical_p=1.0,\n    temp=0.8,\n    repeat_penalty=1.1,\n    repeat_last_n=64,\n    frequency_penalty=0.0,\n    presence_penalty=0.0,\n    mirostat=0,\n    mirostat_tau=5.0,\n    mirostat_eta=0.1,\n    n_batch=8,\n    n_keep=0,\n    interactive=False,\n    antiprompt=[],\n    instruct=False,\n    verbose_prompt=False,\n)\n</code></pre> <p>The generate function from <code>llama.cpp</code></p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>the prompt</p> required <code>n_predict</code> <code>int</code> <p>number of tokens to generate</p> <code>128</code> <code>new_text_callback</code> <code>Callable[[bytes], None]</code> <p>a callback function called when new text is generated, default <code>None</code></p> <code>None</code> <code>n_threads</code> <code>int</code> <p>The number of CPU threads</p> <code>4</code> <code>top_k</code> <code>int</code> <p>top K sampling parameter, &lt;= 0 to use vocab size</p> <code>40</code> <code>top_p</code> <code>float</code> <p>top P sampling parameter, 1.0 = disabled</p> <code>0.95</code> <code>tfs_z</code> <code>float</code> <p>tfs_z sampling parameter, 1.0 = disabled</p> <code>1.0</code> <code>typical_p</code> <code>float</code> <p>typical_p sampling parameter, 1.0 = disabled</p> <code>1.0</code> <code>temp</code> <code>float</code> <p>Temperature, 1.0 = disabled</p> <code>0.8</code> <code>repeat_penalty</code> <code>float</code> <p>repeat penalty sampling parameter, 1.0 = disabled</p> <code>1.1</code> <code>repeat_last_n</code> <code>int</code> <p>last n tokens to penalize (0 = disable penalty, -1 = context size)</p> <code>64</code> <code>frequency_penalty</code> <code>float</code> <p>0.0 = disabled</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>0.0 = disabled</p> <code>0.0</code> <code>mirostat</code> <code>int</code> <p>0 = disabled, 1 = mirostat, 2 = mirostat 2.0</p> <code>0</code> <code>mirostat_tau</code> <code>int</code> <p>target entropy</p> <code>5.0</code> <code>mirostat_eta</code> <code>int</code> <p>learning rate</p> <code>0.1</code> <code>n_batch</code> <code>int</code> <p>GPT params n_batch</p> <code>8</code> <code>n_keep</code> <code>int</code> <p>GPT params n_keep</p> <code>0</code> <code>interactive</code> <code>bool</code> <p>interactive communication</p> <code>False</code> <code>antiprompt</code> <code>List</code> <p>list of anti prompts</p> <code>[]</code> <code>instruct</code> <code>bool</code> <p>Activate instruct mode</p> <code>False</code> <code>verbose_prompt</code> <code>bool</code> <p>verbose prompt</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>the new generated text</p> Source code in <code>pyllamacpp/model.py</code> <pre><code>def cpp_generate(self, prompt: str,\n                 n_predict: int = 128,\n                 new_text_callback: Callable[[bytes], None] = None,\n                 n_threads: int = 4,\n                 top_k: int = 40,\n                 top_p: float = 0.95,\n                 tfs_z: float = 1.00,\n                 typical_p: float = 1.00,\n                 temp: float = 0.8,\n                 repeat_penalty: float = 1.10,\n                 repeat_last_n: int = 64,\n                 frequency_penalty: float = 0.00,\n                 presence_penalty: float = 0.00,\n                 mirostat: int = 0,\n                 mirostat_tau: int = 5.00,\n                 mirostat_eta: int = 0.1,\n                 n_batch: int = 8,\n                 n_keep: int = 0,\n                 interactive: bool = False,\n                 antiprompt: List = [],\n                 instruct: bool = False,\n                 verbose_prompt: bool = False,\n                 ) -&gt; str:\n\"\"\"\n    The generate function from `llama.cpp`\n\n    :param prompt: the prompt\n    :param n_predict: number of tokens to generate\n    :param new_text_callback: a callback function called when new text is generated, default `None`\n    :param n_threads: The number of CPU threads\n    :param top_k: top K sampling parameter, &lt;= 0 to use vocab size\n    :param top_p: top P sampling parameter, 1.0 = disabled\n    :param tfs_z: tfs_z sampling parameter, 1.0 = disabled\n    :param typical_p: typical_p sampling parameter, 1.0 = disabled\n    :param temp: Temperature, 1.0 = disabled\n    :param repeat_penalty: repeat penalty sampling parameter, 1.0 = disabled\n    :param repeat_last_n: last n tokens to penalize (0 = disable penalty, -1 = context size)\n    :param frequency_penalty: 0.0 = disabled\n    :param presence_penalty: 0.0 = disabled\n    :param mirostat: 0 = disabled, 1 = mirostat, 2 = mirostat 2.0\n    :param mirostat_tau: target entropy\n    :param mirostat_eta: learning rate\n    :param n_batch: GPT params n_batch\n    :param n_keep: GPT params n_keep\n    :param interactive: interactive communication\n    :param antiprompt: list of anti prompts\n    :param instruct: Activate instruct mode\n    :param verbose_prompt: verbose prompt\n    :return: the new generated text\n    \"\"\"\n    self.gpt_params.prompt = prompt\n    self.gpt_params.n_predict = n_predict\n    self.gpt_params.n_threads = n_threads\n    self.gpt_params.top_k = top_k\n    self.gpt_params.top_p = top_p\n    self.gpt_params.tfs_z = tfs_z\n    self.gpt_params.typical_p = typical_p\n    self.gpt_params.temp = temp\n    self.gpt_params.repeat_penalty = repeat_penalty\n    self.gpt_params.repeat_last_n = repeat_last_n\n    self.gpt_params.frequency_penalty = frequency_penalty\n    self.gpt_params.presence_penalty = presence_penalty\n    self.gpt_params.mirostat = mirostat\n    self.gpt_params.mirostat_tau = mirostat_tau\n    self.gpt_params.mirostat_eta = mirostat_eta\n    self.gpt_params.n_batch = n_batch\n    self.gpt_params.n_keep = n_keep\n    self.gpt_params.interactive = interactive\n    self.gpt_params.antiprompt = antiprompt\n    self.gpt_params.instruct = instruct\n    self.gpt_params.verbose_prompt = verbose_prompt\n\n    # assign new_text_callback\n    self.res = \"\"\n    Model._new_text_callback = new_text_callback\n\n    # run the prediction\n    pp.llama_generate(self._ctx, self.gpt_params, self._call_new_text_callback)\n    return self.res\n</code></pre>"},{"location":"#pyllamacpp.model.Model.get_params","title":"get_params  <code>staticmethod</code>","text":"<pre><code>get_params(params)\n</code></pre> <p>Returns a <code>dict</code> representation of the params</p> <p>Returns:</p> Type Description <code>dict</code> <p>params dict</p> Source code in <code>pyllamacpp/model.py</code> <pre><code>@staticmethod\ndef get_params(params) -&gt; dict:\n\"\"\"\n    Returns a `dict` representation of the params\n    :return: params dict\n    \"\"\"\n    res = {}\n    for param in dir(params):\n        if param.startswith('__'):\n            continue\n        res[param] = getattr(params, param)\n    return res\n</code></pre>"},{"location":"#pyllamacpp.utils","title":"pyllamacpp.utils","text":"<p>Helper functions</p>"},{"location":"#pyllamacpp.utils.llama_to_ggml","title":"llama_to_ggml","text":"<pre><code>llama_to_ggml(dir_model, ftype=1)\n</code></pre> <p>A helper function to convert LLaMa Pytorch models to ggml, same exact script as <code>convert-pth-to-ggml.py</code> from llama.cpp repository, copied here for convinience purposes only!</p> <p>Parameters:</p> Name Type Description Default <code>dir_model</code> <code>str</code> <p>llama model directory</p> required <code>ftype</code> <code>int</code> <p>0 or 1, 0-&gt; f32, 1-&gt; f16</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>ggml model path</p> Source code in <code>pyllamacpp/utils.py</code> <pre><code>def llama_to_ggml(dir_model: str, ftype: int = 1) -&gt; str:\n\"\"\"\n    A helper function to convert LLaMa Pytorch models to ggml,\n    same exact script as `convert-pth-to-ggml.py` from [llama.cpp](https://github.com/ggerganov/llama.cpp)\n    repository, copied here for convinience purposes only!\n\n    :param dir_model: llama model directory\n    :param ftype: 0 or 1, 0-&gt; f32, 1-&gt; f16\n    :return: ggml model path\n    \"\"\"\n    # output in the same directory as the model\n    assert ftype in [0, 1], f\"ftype should be in [0,1], 0-&gt; f32, 1-&gt; f16\"\n\n    fname_hparams = str((Path(dir_model) / \"params.json\").absolute())\n    fname_tokenizer = str((Path(dir_model).parent / \"tokenizer.model\").absolute())\n\n    def get_n_parts(dim):\n        if dim == 4096:\n            return 1\n        elif dim == 5120:\n            return 2\n        elif dim == 6656:\n            return 4\n        elif dim == 8192:\n            return 8\n        else:\n            print(\"Invalid dim: \" + str(dim))\n            sys.exit(1)\n\n    # possible data types\n    #   ftype == 0 -&gt; float32\n    #   ftype == 1 -&gt; float16\n    #\n    # map from ftype to string\n    ftype_str = [\"f32\", \"f16\"]\n\n    with open(fname_hparams, \"r\") as f:\n        hparams = json.load(f)\n\n    tokenizer = SentencePieceProcessor(fname_tokenizer)\n\n    hparams.update({\"vocab_size\": tokenizer.vocab_size()})\n\n    n_parts = get_n_parts(hparams[\"dim\"])\n\n    print(hparams)\n    print('n_parts = ', n_parts)\n\n    for p in range(n_parts):\n        print('Processing part ', p)\n\n        # fname_model = dir_model + \"/consolidated.00.pth\"\n\n        fname_model = str(Path(dir_model) / f\"consolidated.0{str(p)}.pth\")\n        fname_out = str(Path(dir_model) / f\"ggml-model-{ftype_str[ftype]}.bin\")\n        if (p &gt; 0):\n            fname_out = str(Path(dir_model) / f\"ggml-model-{ftype_str[ftype]}.bin.{str(p)}\")\n\n        model = torch.load(fname_model, map_location=\"cpu\")\n\n        fout = open(fname_out, \"wb\")\n\n        fout.write(struct.pack(\"i\", 0x67676d6c))  # magic: ggml in hex\n        fout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))\n        fout.write(struct.pack(\"i\", hparams[\"dim\"]))\n        fout.write(struct.pack(\"i\", hparams[\"multiple_of\"]))\n        fout.write(struct.pack(\"i\", hparams[\"n_heads\"]))\n        fout.write(struct.pack(\"i\", hparams[\"n_layers\"]))\n        fout.write(struct.pack(\"i\", hparams[\"dim\"] // hparams[\"n_heads\"]))  # rot (obsolete)\n        fout.write(struct.pack(\"i\", ftype))\n\n        # Is this correct??\n        for i in range(32000):\n            if tokenizer.is_unknown(i):\n                # \"&lt;unk&gt;\" token (translated as ??)\n                text = \" \\u2047 \".encode(\"utf-8\")\n                fout.write(struct.pack(\"i\", len(text)))\n                fout.write(text)\n            elif tokenizer.is_control(i):\n                # \"&lt;s&gt;\"/\"&lt;/s&gt;\" tokens\n                fout.write(struct.pack(\"i\", 0))\n            elif tokenizer.is_byte(i):\n                # \"&lt;U+XX&gt;\" tokens (which may be invalid UTF-8)\n                piece = tokenizer.id_to_piece(i)\n                if len(piece) != 6:\n                    print(\"Invalid token: \" + piece)\n                    sys.exit(1)\n                byte_value = int(piece[3:-1], 16)\n                fout.write(struct.pack(\"i\", 1))\n                fout.write(struct.pack(\"B\", byte_value))\n            else:\n                # normal token. Uses U+2581 (LOWER ONE EIGHTH BLOCK) to represent spaces.\n                text = tokenizer.id_to_piece(i).replace(\"\\u2581\", \" \").encode(\"utf-8\")\n                fout.write(struct.pack(\"i\", len(text)))\n                fout.write(text)\n\n        for k, v in model.items():\n            name = k\n            shape = v.shape\n\n            # skip layers.X.attention.inner_attention.rope.freqs\n            if name[-5:] == \"freqs\":\n                continue\n\n            print(\"Processing variable: \" + name + \" with shape: \", shape, \" and type: \", v.dtype)\n\n            # data = tf.train.load_variable(dir_model, name).squeeze()\n            data = v.numpy().squeeze()\n            n_dims = len(data.shape);\n\n            # for efficiency - transpose some matrices\n            # \"model/h.*/attn/c_attn/w\"\n            # \"model/h.*/attn/c_proj/w\"\n            # \"model/h.*/mlp/c_fc/w\"\n            # \"model/h.*/mlp/c_proj/w\"\n            # if name[-14:] == \"/attn/c_attn/w\" or \\\n            #   name[-14:] == \"/attn/c_proj/w\" or \\\n            #   name[-11:] == \"/mlp/c_fc/w\" or \\\n            #   name[-13:] == \"/mlp/c_proj/w\":\n            #    print(\"  Transposing\")\n            #    data = data.transpose()\n\n            dshape = data.shape\n\n            # default type is fp16\n            ftype_cur = 1\n            if ftype == 0 or n_dims == 1:\n                print(\"  Converting to float32\")\n                data = data.astype(np.float32)\n                ftype_cur = 0\n\n            # header\n            sname = name.encode('utf-8')\n            fout.write(struct.pack(\"iii\", n_dims, len(sname), ftype_cur))\n            for i in range(n_dims):\n                fout.write(struct.pack(\"i\", dshape[n_dims - 1 - i]))\n            fout.write(sname);\n\n            # data\n            data.tofile(fout)\n\n        # I hope this deallocates the memory ..\n        model = None\n\n        fout.close()\n\n        print(\"Done. Output file: \" + fname_out + \", (part \", p, \")\")\n        print(\"\")\n        return fname_out\n</code></pre>"},{"location":"#pyllamacpp.utils.quantize","title":"quantize","text":"<pre><code>quantize(ggml_model_path, output_model_path=None, itype=2)\n</code></pre> <p>Qunatizes the ggml model.</p> <p>Parameters:</p> Name Type Description Default <code>ggml_model_path</code> <code>str</code> <p>path of the ggml model</p> required <code>output_model_path</code> <code>str</code> <p>output file path for the qunatized model</p> <code>None</code> <code>itype</code> <code>int</code> <p>quantization type: 2 -&gt; Q4_0, 3 -&gt; Q4_1</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>quantized model path</p> Source code in <code>pyllamacpp/utils.py</code> <pre><code>def quantize(ggml_model_path: str, output_model_path: str = None, itype: int = 2) -&gt; str:\n\"\"\"\n    Qunatizes the ggml model.\n\n    :param ggml_model_path: path of the ggml model\n    :param output_model_path: output file path for the qunatized model\n    :param itype: quantization type: 2 -&gt; Q4_0, 3 -&gt; Q4_1\n    :return: quantized model path\n    \"\"\"\n    if output_model_path is None:\n        output_model_path = ggml_model_path + f'-q4_{0 if itype == 2 else 1}.bin'\n    logging.info(\"Quantization will start soon ... (This my take a while)\")\n    pp.llama_quantize(ggml_model_path, output_model_path, itype)\n    logging.info(f\"Quantized model is created successfully {output_model_path}\")\n    return output_model_path\n</code></pre>"},{"location":"#_pyllamacpp","title":"_pyllamacpp","text":""},{"location":"#_pyllamacpp--pyllamacpp-python-binding-for-llamacpp","title":"PyLLaMACpp: Python binding for llama.cpp","text":"<p>.. currentmodule:: _pyllamacpp</p> <p>.. autosummary::    :toctree: _generate</p>"},{"location":"#_pyllamacpp.LLaMAModel","title":"LLaMAModel","text":"<pre><code>LLaMAModel()\n</code></pre> <p>         Bases: <code>pybind11_object</code></p> <p>init(self: _pyllamacpp.LLaMAModel, arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.gpt_params, arg2: int) -&gt; None</p>"},{"location":"#_pyllamacpp.LLaMAModel.generate","title":"generate  <code>method descriptor</code>","text":"<pre><code>generate()\n</code></pre> <p>generate(self: _pyllamacpp.LLaMAModel, arg0: _pyllamacpp.gpt_params) -&gt; int</p>"},{"location":"#_pyllamacpp.LLaMAModel.setup","title":"setup  <code>method descriptor</code>","text":"<pre><code>setup()\n</code></pre> <p>setup(self: _pyllamacpp.LLaMAModel, arg0: _pyllamacpp.gpt_params) -&gt; int</p>"},{"location":"#_pyllamacpp.LLaMAModel.update_prompt","title":"update_prompt  <code>method descriptor</code>","text":"<pre><code>update_prompt()\n</code></pre> <p>update_prompt(self: _pyllamacpp.LLaMAModel, arg0: _pyllamacpp.gpt_params, arg1: str) -&gt; None</p>"},{"location":"#_pyllamacpp.gpt_params","title":"gpt_params","text":"<pre><code>gpt_params()\n</code></pre> <p>         Bases: <code>pybind11_object</code></p> <p>init(self: _pyllamacpp.gpt_params) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_context","title":"llama_context","text":"<pre><code>llama_context(*args, **kwargs)\n</code></pre> <p>         Bases: <code>pybind11_object</code></p> <p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"#_pyllamacpp.llama_context_params","title":"llama_context_params","text":"<pre><code>llama_context_params()\n</code></pre> <p>         Bases: <code>pybind11_object</code></p> <p>init(self: _pyllamacpp.llama_context_params) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_ftype","title":"llama_ftype","text":"<pre><code>llama_ftype()\n</code></pre> <p>         Bases: <code>pybind11_object</code></p> <p>Members:</p> <p>LLAMA_FTYPE_ALL_F32</p> <p>LLAMA_FTYPE_MOSTLY_F16</p> <p>LLAMA_FTYPE_MOSTLY_Q4_0</p> <p>LLAMA_FTYPE_MOSTLY_Q4_1</p> <p>LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16</p> <p>LLAMA_FTYPE_MOSTLY_Q8_0</p> <p>LLAMA_FTYPE_MOSTLY_Q5_0</p> <p>LLAMA_FTYPE_MOSTLY_Q5_1</p> <p>init(self: _pyllamacpp.llama_ftype, value: int) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_ftype.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>name(self: handle) -&gt; str</p>"},{"location":"#_pyllamacpp.llama_token_data","title":"llama_token_data","text":"<pre><code>llama_token_data()\n</code></pre> <p>         Bases: <code>pybind11_object</code></p> <p>init(self: _pyllamacpp.llama_token_data) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_token_data_array","title":"llama_token_data_array","text":"<pre><code>llama_token_data_array()\n</code></pre> <p>         Bases: <code>pybind11_object</code></p> <p>init(self: _pyllamacpp.llama_token_data_array) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_apply_lora_from_file","title":"llama_apply_lora_from_file  <code>builtin</code>","text":"<pre><code>llama_apply_lora_from_file()\n</code></pre> <p>llama_apply_lora_from_file(arg0: _pyllamacpp.llama_context, arg1: str, arg2: str, arg3: int) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_context_default_params","title":"llama_context_default_params  <code>builtin</code>","text":"<pre><code>llama_context_default_params()\n</code></pre> <p>llama_context_default_params() -&gt; _pyllamacpp.llama_context_params</p>"},{"location":"#_pyllamacpp.llama_eval","title":"llama_eval  <code>builtin</code>","text":"<pre><code>llama_eval()\n</code></pre> <p>llama_eval(arg0: _pyllamacpp.llama_context, arg1: numpy.ndarray[numpy.int32], arg2: int, arg3: int, arg4: int) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_free","title":"llama_free  <code>builtin</code>","text":"<pre><code>llama_free()\n</code></pre> <p>llama_free(arg0: _pyllamacpp.llama_context) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_generate","title":"llama_generate  <code>builtin</code>","text":"<pre><code>llama_generate()\n</code></pre> <p>llama_generate(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.gpt_params, arg2: function) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_get_embeddings","title":"llama_get_embeddings  <code>builtin</code>","text":"<pre><code>llama_get_embeddings()\n</code></pre> <p>llama_get_embeddings(arg0: _pyllamacpp.llama_context) -&gt; float</p>"},{"location":"#_pyllamacpp.llama_get_kv_cache_token_count","title":"llama_get_kv_cache_token_count  <code>builtin</code>","text":"<pre><code>llama_get_kv_cache_token_count()\n</code></pre> <p>llama_get_kv_cache_token_count(arg0: _pyllamacpp.llama_context) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_get_logits","title":"llama_get_logits  <code>builtin</code>","text":"<pre><code>llama_get_logits()\n</code></pre> <p>llama_get_logits(arg0: _pyllamacpp.llama_context) -&gt; float</p>"},{"location":"#_pyllamacpp.llama_get_state_size","title":"llama_get_state_size  <code>builtin</code>","text":"<pre><code>llama_get_state_size()\n</code></pre> <p>llama_get_state_size(arg0: _pyllamacpp.llama_context) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_init_from_file","title":"llama_init_from_file  <code>builtin</code>","text":"<pre><code>llama_init_from_file()\n</code></pre> <p>llama_init_from_file(arg0: str, arg1: _pyllamacpp.llama_context_params) -&gt; _pyllamacpp.llama_context</p>"},{"location":"#_pyllamacpp.llama_load_session_file","title":"llama_load_session_file  <code>builtin</code>","text":"<pre><code>llama_load_session_file()\n</code></pre> <p>llama_load_session_file(arg0: _pyllamacpp.llama_context, arg1: str, arg2: numpy.ndarray[numpy.int32], arg3: int, arg4: int) -&gt; bool</p>"},{"location":"#_pyllamacpp.llama_mlock_supported","title":"llama_mlock_supported  <code>builtin</code>","text":"<pre><code>llama_mlock_supported()\n</code></pre> <p>llama_mlock_supported() -&gt; bool</p>"},{"location":"#_pyllamacpp.llama_mmap_supported","title":"llama_mmap_supported  <code>builtin</code>","text":"<pre><code>llama_mmap_supported()\n</code></pre> <p>llama_mmap_supported() -&gt; bool</p>"},{"location":"#_pyllamacpp.llama_model_quantize","title":"llama_model_quantize  <code>builtin</code>","text":"<pre><code>llama_model_quantize()\n</code></pre> <p>llama_model_quantize(args, *kwargs) Overloaded function.</p> <ol> <li> <p>llama_model_quantize(arg0: str, arg1: str, arg2: _pyllamacpp.llama_ftype, arg3: int) -&gt; int</p> </li> <li> <p>llama_model_quantize(arg0: str, arg1: str, arg2: _pyllamacpp.llama_ftype, arg3: int) -&gt; int</p> </li> </ol>"},{"location":"#_pyllamacpp.llama_n_ctx","title":"llama_n_ctx  <code>builtin</code>","text":"<pre><code>llama_n_ctx()\n</code></pre> <p>llama_n_ctx(arg0: _pyllamacpp.llama_context) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_n_embd","title":"llama_n_embd  <code>builtin</code>","text":"<pre><code>llama_n_embd()\n</code></pre> <p>llama_n_embd(arg0: _pyllamacpp.llama_context) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_n_vocab","title":"llama_n_vocab  <code>builtin</code>","text":"<pre><code>llama_n_vocab()\n</code></pre> <p>llama_n_vocab(arg0: _pyllamacpp.llama_context) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_print_system_info","title":"llama_print_system_info  <code>builtin</code>","text":"<pre><code>llama_print_system_info()\n</code></pre> <p>llama_print_system_info() -&gt; str</p>"},{"location":"#_pyllamacpp.llama_print_timings","title":"llama_print_timings  <code>builtin</code>","text":"<pre><code>llama_print_timings()\n</code></pre> <p>llama_print_timings(arg0: _pyllamacpp.llama_context) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_reset_timings","title":"llama_reset_timings  <code>builtin</code>","text":"<pre><code>llama_reset_timings()\n</code></pre> <p>llama_reset_timings(arg0: _pyllamacpp.llama_context) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_sample_frequency_and_presence_penalties","title":"llama_sample_frequency_and_presence_penalties  <code>builtin</code>","text":"<pre><code>llama_sample_frequency_and_presence_penalties()\n</code></pre> <p>llama_sample_frequency_and_presence_penalties(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array, arg2: numpy.ndarray[numpy.int32], arg3: int, arg4: float, arg5: float) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_sample_repetition_penalty","title":"llama_sample_repetition_penalty  <code>builtin</code>","text":"<pre><code>llama_sample_repetition_penalty()\n</code></pre> <p>llama_sample_repetition_penalty(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array, arg2: numpy.ndarray[numpy.int32], arg3: int, arg4: float) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_sample_softmax","title":"llama_sample_softmax  <code>builtin</code>","text":"<pre><code>llama_sample_softmax()\n</code></pre> <p>llama_sample_softmax(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_sample_tail_free","title":"llama_sample_tail_free  <code>builtin</code>","text":"<pre><code>llama_sample_tail_free()\n</code></pre> <p>llama_sample_tail_free(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array, arg2: float, arg3: int) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_sample_temperature","title":"llama_sample_temperature  <code>builtin</code>","text":"<pre><code>llama_sample_temperature()\n</code></pre> <p>llama_sample_temperature(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array, arg2: float) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_sample_token","title":"llama_sample_token  <code>builtin</code>","text":"<pre><code>llama_sample_token()\n</code></pre> <p>llama_sample_token(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_sample_token_greedy","title":"llama_sample_token_greedy  <code>builtin</code>","text":"<pre><code>llama_sample_token_greedy()\n</code></pre> <p>llama_sample_token_greedy(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_sample_token_mirostat","title":"llama_sample_token_mirostat  <code>builtin</code>","text":"<pre><code>llama_sample_token_mirostat()\n</code></pre> <p>llama_sample_token_mirostat(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array, arg2: float, arg3: float, arg4: int, arg5: float) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_sample_token_mirostat_v2","title":"llama_sample_token_mirostat_v2  <code>builtin</code>","text":"<pre><code>llama_sample_token_mirostat_v2()\n</code></pre> <p>llama_sample_token_mirostat_v2(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array, arg2: float, arg3: float, arg4: float) -&gt; int</p>"},{"location":"#_pyllamacpp.llama_sample_top_k","title":"llama_sample_top_k  <code>builtin</code>","text":"<pre><code>llama_sample_top_k()\n</code></pre> <p>llama_sample_top_k(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array, arg2: int, arg3: int) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_sample_top_p","title":"llama_sample_top_p  <code>builtin</code>","text":"<pre><code>llama_sample_top_p()\n</code></pre> <p>llama_sample_top_p(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array, arg2: float, arg3: int) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_sample_typical","title":"llama_sample_typical  <code>builtin</code>","text":"<pre><code>llama_sample_typical()\n</code></pre> <p>llama_sample_typical(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.llama_token_data_array, arg2: float, arg3: int) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_save_session_file","title":"llama_save_session_file  <code>builtin</code>","text":"<pre><code>llama_save_session_file()\n</code></pre> <p>llama_save_session_file(arg0: _pyllamacpp.llama_context, arg1: str, arg2: numpy.ndarray[numpy.int32], arg3: int) -&gt; bool</p>"},{"location":"#_pyllamacpp.llama_set_rng_seed","title":"llama_set_rng_seed  <code>builtin</code>","text":"<pre><code>llama_set_rng_seed()\n</code></pre> <p>llama_set_rng_seed(arg0: _pyllamacpp.llama_context, arg1: int) -&gt; None</p>"},{"location":"#_pyllamacpp.llama_token_bos","title":"llama_token_bos  <code>builtin</code>","text":"<pre><code>llama_token_bos()\n</code></pre> <p>llama_token_bos() -&gt; int</p>"},{"location":"#_pyllamacpp.llama_token_eos","title":"llama_token_eos  <code>builtin</code>","text":"<pre><code>llama_token_eos()\n</code></pre> <p>llama_token_eos() -&gt; int</p>"},{"location":"#_pyllamacpp.llama_token_nl","title":"llama_token_nl  <code>builtin</code>","text":"<pre><code>llama_token_nl()\n</code></pre> <p>llama_token_nl() -&gt; int</p>"},{"location":"#_pyllamacpp.llama_token_to_str","title":"llama_token_to_str  <code>builtin</code>","text":"<pre><code>llama_token_to_str()\n</code></pre> <p>llama_token_to_str(arg0: _pyllamacpp.llama_context, arg1: int) -&gt; bytes</p>"},{"location":"#_pyllamacpp.llama_tokenize","title":"llama_tokenize  <code>builtin</code>","text":"<pre><code>llama_tokenize()\n</code></pre> <p>llama_tokenize(arg0: _pyllamacpp.llama_context, arg1: str, arg2: bool) -&gt; List[int]</p>"},{"location":"#_pyllamacpp.llama_tokens_to_str","title":"llama_tokens_to_str  <code>builtin</code>","text":"<pre><code>llama_tokens_to_str()\n</code></pre> <p>llama_tokens_to_str(arg0: _pyllamacpp.llama_context, arg1: numpy.ndarray[numpy.int32]) -&gt; str</p>"},{"location":"#_pyllamacpp.sample_next_token","title":"sample_next_token  <code>builtin</code>","text":"<pre><code>sample_next_token()\n</code></pre> <p>sample_next_token(arg0: _pyllamacpp.llama_context, arg1: _pyllamacpp.gpt_params, arg2: List[int]) -&gt; int</p>"}]}