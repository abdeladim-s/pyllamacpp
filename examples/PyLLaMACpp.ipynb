{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyLLaMACpp Notebook\n",
        "\n",
        "## The notebook runs [llama.cpp](https://github.com/ggerganov/llama.cpp) using [pyllamacpp](https://github.com/abdeladim-s/pyllamacpp)."
      ],
      "metadata": {
        "id": "tkIG2iDM6Roi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the `ggml` models\n",
        "\n"
      ],
      "metadata": {
        "id": "DoxQH94rM8sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this cell is to upload the models to Colab\n",
        "\n",
        "!wget https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GGML/resolve/main/WizardLM-7B-uncensored.ggmlv3.q4_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNHZRTCzDXQg",
        "outputId": "2f643506-8bc7-49d3-f6bd-5a6e9f2ca1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-01 00:41:23--  https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GGML/resolve/main/WizardLM-7B-uncensored.ggmlv3.q4_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 99.84.191.107, 99.84.191.42, 99.84.191.66, ...\n",
            "Connecting to huggingface.co (huggingface.co)|99.84.191.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/c0/cd/c0cd768b4cd58780ae60ca18240a853723360aac1874854c9e07bc87d943ee47/82efddf53c218663d382d8f5c6e4f664391b13025773cc63b65e28c7a987ae97?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27WizardLM-7B-uncensored.ggmlv3.q4_0.bin%3B+filename%3D%22WizardLM-7B-uncensored.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1685839284&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2MwL2NkL2MwY2Q3NjhiNGNkNTg3ODBhZTYwY2ExODI0MGE4NTM3MjMzNjBhYWMxODc0ODU0YzllMDdiYzg3ZDk0M2VlNDcvODJlZmRkZjUzYzIxODY2M2QzODJkOGY1YzZlNGY2NjQzOTFiMTMwMjU3NzNjYzYzYjY1ZTI4YzdhOTg3YWU5Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODU4MzkyODR9fX1dfQ__&Signature=WPUp1kplI4gwxpVWnuAFkQmQTjbSZtvAbFq%7EPUhYrAsNVo4bL8kLexTk-EfKilbgZLyKxJtj9WtQmm9u3gTgshrA1VVmhZm9OonqGpuzEdtVji8dssgaa-dLNkK7M455O8J7jvA00irWNQyWpO8WRkknMCHfGHZyNjUVQcsdpSL3tNeBRQBthIx808HjvN5HLw6aET3jAgGLOvaUpCzz88KzGKjDOcF%7E19Mrp9q5esf%7EwRRgOJWvKDp2XzW7oi00P%7EcvNhzNreRUnGt5%7ErmxeFxg6%7EL138kCoL9YIigLygAv4k4xUw6ycNtIWSNwuttixaIlWRkeTAXNecYafjLL3w__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-06-01 00:41:23--  https://cdn-lfs.huggingface.co/repos/c0/cd/c0cd768b4cd58780ae60ca18240a853723360aac1874854c9e07bc87d943ee47/82efddf53c218663d382d8f5c6e4f664391b13025773cc63b65e28c7a987ae97?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27WizardLM-7B-uncensored.ggmlv3.q4_0.bin%3B+filename%3D%22WizardLM-7B-uncensored.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1685839284&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2MwL2NkL2MwY2Q3NjhiNGNkNTg3ODBhZTYwY2ExODI0MGE4NTM3MjMzNjBhYWMxODc0ODU0YzllMDdiYzg3ZDk0M2VlNDcvODJlZmRkZjUzYzIxODY2M2QzODJkOGY1YzZlNGY2NjQzOTFiMTMwMjU3NzNjYzYzYjY1ZTI4YzdhOTg3YWU5Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODU4MzkyODR9fX1dfQ__&Signature=WPUp1kplI4gwxpVWnuAFkQmQTjbSZtvAbFq%7EPUhYrAsNVo4bL8kLexTk-EfKilbgZLyKxJtj9WtQmm9u3gTgshrA1VVmhZm9OonqGpuzEdtVji8dssgaa-dLNkK7M455O8J7jvA00irWNQyWpO8WRkknMCHfGHZyNjUVQcsdpSL3tNeBRQBthIx808HjvN5HLw6aET3jAgGLOvaUpCzz88KzGKjDOcF%7E19Mrp9q5esf%7EwRRgOJWvKDp2XzW7oi00P%7EcvNhzNreRUnGt5%7ErmxeFxg6%7EL138kCoL9YIigLygAv4k4xUw6ycNtIWSNwuttixaIlWRkeTAXNecYafjLL3w__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.49, 108.138.64.111, 108.138.64.121, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3791729792 (3.5G) [application/octet-stream]\n",
            "Saving to: ‘WizardLM-7B-uncensored.ggmlv3.q4_0.bin.1’\n",
            "\n",
            "WizardLM-7B-uncenso 100%[===================>]   3.53G  41.4MB/s    in 74s     \n",
            "\n",
            "2023-06-01 00:42:37 (49.1 MB/s) - ‘WizardLM-7B-uncensored.ggmlv3.q4_0.bin.1’ saved [3791729792/3791729792]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "92NsaMgMMli0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyllamacpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFcAn6y_BPi2",
        "outputId": "ff6bf093-bc38-44fe-a923-8c3725e8485b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyllamacpp\n",
            "  Downloading pyllamacpp-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyllamacpp\n",
            "Successfully installed pyllamacpp-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model"
      ],
      "metadata": {
        "id": "7Kv0oFkWNtUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/WizardLM-7B-uncensored.ggmlv3.q4_0.bin\" # @param {type: \"string\"}"
      ],
      "metadata": {
        "id": "1d-Q1XZEut11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple generation"
      ],
      "metadata": {
        "id": "w_6qE_GROg9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyllamacpp.model import Model\n",
        "\n",
        "model = Model(model_path=model_path, n_ctx=512)\n",
        "\n",
        "prompt = \"tell me a joke ?\"  # @param {type: 'string'}\n",
        "n_threads = 2  # @param {type: 'integer'}\n",
        "n_predict = 45  # @param {type: 'integer'}\n",
        "repeat_last_n = 64  # @param {type: 'integer'}\n",
        "top_k = 40  # @param {type: 'integer'}\n",
        "top_p = 0.95  # @param {type: 'number'}\n",
        "temp = 0.6  # @param {type: 'number'}\n",
        "repeat_penalty = 1.3  # @param {type: 'number'}\n",
        "\n",
        "\n",
        "\n",
        "for token in model.generate(prompt,  \n",
        "                                n_threads=n_threads,\n",
        "                                n_predict=n_predict,\n",
        "                                repeat_last_n=repeat_last_n,\n",
        "                                top_k=top_k,\n",
        "                                top_p=top_p,\n",
        "                                temp=temp,\n",
        "                                repeat_penalty=repeat_penalty):\n",
        "  print(token, end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUAKj4cvFQ9f",
        "outputId": "71d41ed4-e65e-4aee-a37b-0bf54c840c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I'm happy to help! Here's one:\n",
            "Why did the tomato turn red? Because it saw the salad dressing."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat with bob"
      ],
      "metadata": {
        "id": "Rtqrj1CvbvZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyllamacpp.model import Model\n",
        "\n",
        "prompt_context = \"\"\"Act as Bob. Bob is helpful, kind, honest,\n",
        "and never fails to answer the User's requests immediately and with precision. \n",
        "\n",
        "User: Nice to meet you Bob!\n",
        "Bob: Welcome! I'm here to assist you with anything you need. What can I do for you today?\n",
        "\"\"\"\n",
        "\n",
        "prompt_prefix = \"\\nUser:\"\n",
        "prompt_suffix = \"\\nBob:\"\n",
        "\n",
        "model = Model(model_path,\n",
        "              n_ctx=512,\n",
        "              prompt_context=prompt_context,\n",
        "              prompt_prefix=prompt_prefix,\n",
        "              prompt_suffix=prompt_suffix)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        prompt = input(\"User: \")\n",
        "        if prompt == '':\n",
        "            continue\n",
        "        print(f\"Bob: \", end='')\n",
        "        for token in model.generate(prompt,\n",
        "                                    antiprompt='User:',  \n",
        "                                    n_threads=2,\n",
        "                                    n_predict=24,\n",
        "                                    repeat_penalty=1.0,):\n",
        "            print(f\"{token}\", end='', flush=True)\n",
        "        print()\n",
        "    except KeyboardInterrupt:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCe2fbo_bzRQ",
        "outputId": "a7dd88fd-635d-44a4-88b0-aa97eb47c3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hello, what is your name ?\n",
            "Bob:  My name is Bob. How can I assist you today?\n",
            "User: what is the largest city in the world ?\n",
            "Bob:  The largest city in the world is Tokyo, Japan.\n",
            "User: why it is the largest ?\n",
            "Bob:  Tokyo became the largest city in the world due to its rapid growth and expansion in the mid-20th century.\n",
            "User: ok, thank you!\n",
            "Bob:  You're welcome! Is there anything else I can help you with?\n"
          ]
        }
      ]
    }
  ]
}